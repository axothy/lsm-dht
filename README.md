# 2024-highload-dht
Курсовой проект курса «Разработка high-load систем» [Корпоративной магистерской программы «Распределённые веб-сервисы / Web scale systems»](https://dws.itmo.ru/).

## Этап 1. HTTP + storage 

### Make
Так можно запустить тесты:
```
$ ./gradlew test
```

А вот так -- сервер:
```
$ ./gradlew run
```

### Develop
Откройте в IDE -- [IntelliJ IDEA Community Edition](https://www.jetbrains.com/idea/) нам будет достаточно.

**ВНИМАНИЕ!** При запуске тестов или сервера в IDE необходимо передавать Java опцию `-Xmx128m`.

В своём Java package `ru.vk.itmo.test.<username>` реализуйте интерфейсы [`Service`](src/main/java/ru/vk/itmo/Service.java) и [`ServiceFactory.Factory`](src/main/java/ru/vk/itmo/test/ServiceFactory.java) и поддержите следующий HTTP REST API протокол:
* HTTP `GET /v0/entity?id=<ID>` -- получить данные по ключу `<ID>`. Возвращает `200 OK` и данные или `404 Not Found`.
* HTTP `PUT /v0/entity?id=<ID>` -- создать/перезаписать (upsert) данные по ключу `<ID>`. Возвращает `201 Created`.
* HTTP `DELETE /v0/entity?id=<ID>` -- удалить данные по ключу `<ID>`. Возвращает `202 Accepted`.

Используйте свою реализацию `Dao` из предыдущего курса `2023-nosql-lsm` или референсную реализацию, если своей нет.

Проведите нагрузочное тестирование с помощью [wrk2](https://github.com/giltene/wrk2) в **одно соединение**:
* `PUT` запросами на **стабильной** нагрузке (`wrk2` должен обеспечивать заданный с помощью `-R` rate запросов) **ниже точки разладки**
* `GET` запросами на **стабильной** нагрузке по **наполненной** БД **ниже точки разладки**

Нагрузочное тестирование и профилирование должны **проводиться в одинаковых условиях** (при одинаковой нагрузке на CPU). А почему не `curl`/F5, можно узнать [здесь](http://highscalability.com/blog/2015/10/5/your-load-generator-is-probably-lying-to-you-take-the-red-pi.html) и [здесь](https://www.youtube.com/watch?v=lJ8ydIuPFeU).

Приложите полученный консольный вывод `wrk2` для обоих видов нагрузки.

Отпрофилируйте приложение (CPU и alloc) под `PUT` и `GET` нагрузкой с помощью [async-profiler](https://github.com/async-profiler/async-profiler/).
Приложите SVG-файлы FlameGraph `cpu`/`alloc` для `PUT`/`GET` нагрузки.

**Объясните** результаты нагрузочного тестирования и профилирования и приложите **текстовый отчёт** (в Markdown). Все используемые инструменты были рассмотрены на лекции -- смотрите видео запись.

Продолжайте запускать тесты и исправлять ошибки, не забывая [подтягивать новые тесты и фиксы из `upstream`](https://help.github.com/articles/syncing-a-fork/).
Если заметите ошибку в `upstream`, заводите баг и присылайте pull request ;)

### Report
Когда всё будет готово, присылайте pull request со своей реализацией, результатами профилирования, отчётом с их анализом и проведёнными по результату профилирования оптимизациями на review. На всех этапах **оценивается и код, и анализ (отчёт)** -- без анализа полученных результатов работа оценивается минимальным количеством баллов.
Не забывайте **отвечать на комментарии в PR** (в том числе автоматизированные) и **исправлять замечания**!

# Отчёт по "Этап 1. HTTP + storage"

## UPSERT

Был создан скрипт на Lua для того, чтобы наполнять изначально пустую базу key-value, ключи генерируются уникальные.
В данном исследовании была определена, первым делом, точка разладки при PUT запросах при помощи нагрузочного тестирования. Она составляет ±45 000 rps. Нагрузим наш Server:

```
./wrk -c 1 -d 30 -t 1 -L -R 42500 -s upsert-script.lua http://localhost:8080

Running 30s test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 357.303ms, rate sampling interval: 759ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   468.35ms   71.67ms 584.19ms   51.36%
    Req/Sec    42.12k   840.37    43.32k    69.23%
  Latency Distribution (HdrHistogram - Recorded Latency)
----------------------------------------------------------
  1250617 requests in 30.00s, 79.91MB read
Requests/sec:  41687.74
```


В этом случае сервер почти моментально падает. Немного уменьшим rps, чтобы увидеть деградацию нашего сервиса.

```
./wrk -c 1 -d 30 -t 1 -L -R 40000 -s upsert-script.lua http://localhost:8080

Running 30s test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 94.909ms, rate sampling interval: 528ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     4.61ms   10.91ms  61.95ms   90.11%
    Req/Sec    40.05k   663.79    42.59k    86.49%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%  804.00us
 75.000%    1.20ms
 90.000%   14.79ms
 99.000%   55.39ms
 99.900%   61.38ms
 99.990%   61.92ms
 99.999%   61.98ms
100.000%   61.98ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.014     0.000000            1         1.00
       0.191     0.100000        80074         1.11
       0.344     0.200000       160024         1.25
       0.495     0.300000       240162         1.43
       0.648     0.400000       319965         1.67
       0.804     0.500000       399957         2.00
       0.882     0.550000       440310         2.22
       0.961     0.600000       480280         2.50
       1.037     0.650000       520205         2.86
       1.115     0.700000       560110         3.33
       1.195     0.750000       599900         4.00
       1.253     0.775000       620037         4.44
       2.149     0.800000       639810         5.00
       4.191     0.825000       659836         5.71
       6.235     0.850000       679812         6.67
       8.831     0.875000       699797         8.00
      10.879     0.887500       709800         8.89
      14.791     0.900000       719777        10.00
      19.807     0.912500       729795        11.43
      22.559     0.925000       739798        13.33
      26.399     0.937500       749777        16.00
      27.327     0.943750       754775        17.78
      29.743     0.950000       759774        20.00
      32.895     0.956250       764769        22.86
      37.055     0.962500       769794        26.67
      41.151     0.968750       774812        32.00
      43.359     0.971875       777285        35.56
      45.535     0.975000       779794        40.00
      47.487     0.978125       782267        45.71
      49.087     0.981250       784770        53.33
      51.391     0.984375       787256        64.00
      52.575     0.985938       788565        71.11
      53.631     0.987500       789762        80.00
      54.687     0.989062       791034        91.43
      55.903     0.990625       792286       106.67
      56.991     0.992188       793513       128.00
      57.599     0.992969       794168       142.22
      58.175     0.993750       794767       160.00
      58.719     0.994531       795416       182.86
      59.199     0.995313       796006       213.33
      59.615     0.996094       796647       256.00
      59.871     0.996484       796947       284.44
      60.127     0.996875       797296       320.00
      60.351     0.997266       797567       365.71
      60.607     0.997656       797877       426.67
      60.927     0.998047       798211       512.00
      60.991     0.998242       798348       568.89
      61.055     0.998437       798503       640.00
      61.215     0.998633       798679       731.43
      61.311     0.998828       798849       853.33
      61.407     0.999023       799035      1024.00
      61.439     0.999121       799078      1137.78
      61.535     0.999219       799168      1280.00
      61.567     0.999316       799208      1462.86
      61.663     0.999414       799294      1706.67
      61.727     0.999512       799384      2048.00
      61.759     0.999561       799450      2275.56
      61.759     0.999609       799450      2560.00
      61.823     0.999658       799541      2925.71
      61.823     0.999707       799541      3413.33
      61.855     0.999756       799597      4096.00
      61.855     0.999780       799597      4551.11
      61.855     0.999805       799597      5120.00
      61.887     0.999829       799636      5851.43
      61.887     0.999854       799636      6826.67
      61.919     0.999878       799688      8192.00
      61.919     0.999890       799688      9102.22
      61.919     0.999902       799688     10240.00
      61.919     0.999915       799688     11702.86
      61.951     0.999927       799722     13653.33
      61.951     0.999939       799722     16384.00
      61.951     0.999945       799722     18204.44
      61.951     0.999951       799722     20480.00
      61.951     0.999957       799722     23405.71
      61.951     0.999963       799722     27306.67
      61.983     0.999969       799751     32768.00
      61.983     1.000000       799751          inf
#[Mean    =        4.605, StdDeviation   =       10.905]
#[Max     =       61.952, Total count    =       799751]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  1199929 requests in 30.00s, 76.67MB read
Requests/sec:  39997.94
Transfer/sec:      2.56MB

```

На гистограмме распределении времен ответа, построенной по результатам вывода wrk2 выше, видна деградация:

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage1/upsert-max-rps.png)
<img width="1475" alt="upsert-max-rps" src="https://github.com/axothy/lsm-dht/assets/61747868/49baac85-a678-488e-a4b8-76f11d576bdf">

Таким образом, будем считать в данном исследовании стабильной нагрузку около 30 000 rps. Нагрузим стабильно в течение 120 секунд:

```
./wrk -c 1 -d 120 -t 1 -L -R 30000 -s upsert-script.lua http://localhost:8080
Running 2m test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 6.288ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   810.24us    1.59ms  32.05ms   98.27%
    Req/Sec    31.68k     3.20k   52.89k    80.15%
----------------------------------------------------------
  3599976 requests in 2.00m, 230.02MB read
Requests/sec:  29999.85
Transfer/sec:      1.92MB

```

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage1/upsert-stable-rps.png)
<img width="1463" alt="upsert-stable-rps" src="https://github.com/axothy/lsm-dht/assets/61747868/b6e72514-ce34-4b1e-bfde-5c71387a150e">

Видим что среднее Latency - 0.81 ms, а по перцентилям гистограммы выше видно, что деградации не наблюдалось.


## GET

В данном исследовании проведем нагрузочное тестирование GET запросами. Наполнять БД можно следующим образом:

1) Заполнить только Memtable;
2) Заполнить Memtable и сделать flush в один SSTable;
3) Сделать множественный flush, создав таким образом несколько SSTable.

Интерес представляет из себя последний вариант. Для данного исследования был написан код, который заполняет Dao ключами и значениями с фиксированным количеством SSTable. Пусть будет 100 таблиц на диске. Ключи распределены по таблицам в совершенно случайном порядке. Такое количество таблиц позволит увидеть влияние перебора таблиц для поиска нужного entry в нашем LSM.

А теперь представим, что у нас нет фильтра Блюма. Таблиц на диске может быть очень много, в нашем случае их 100, а перебирать последовательно таблицы для каждого GET и в каждом производить бинарный поиск - как-то не привлекательно. Получается, мы в худшем случае сделаем 100 бинарных поисков!

Оценим, как работает наша база данных без фильтра Блюма на SSTable-ах. В LSM чтение медленее upsert-а, для приличия возьмем нагрузку в 20k rps на 20 секунд:

```
./wrk -c 1 -d 20 -t 1 -L -R 20000 -s get-script.lua http://localhost:8080

Running 20s test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 3414.116ms, rate sampling interval: 11698ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     9.52s     1.83s   12.71s    57.64%
    Req/Sec        nan       nan   0.00      0.00%
----------------------------------------------------------
  145772 requests in 20.00s, 9.49MB read
Requests/sec:   7288.58
Transfer/sec:    485.70KB
```

Detailed Percentile spectrum тут не нужен, и так видно, что база просто моментально захлебнулась таким количеством бинарных поисков.

Возьмем стабильную 7.5k rps
```
./wrk -c 1 -d 20 -t 1 -L -R 7500 -s get-script.lua http://localhost:8080

Running 20s test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 153.675ms, rate sampling interval: 507ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     2.87ms    2.82ms  16.24ms   86.19%
    Req/Sec     7.51k    66.42     7.68k    73.68%
----------------------------------------------------------
  149989 requests in 20.00s, 9.76MB read
Requests/sec:   7499.49
Transfer/sec:    499.91KB
```

Сервис стабильно обрабатывает запросы, максимальный latency составил 16.24ms, средний - 2.87ms. Вроде хорошо - но выше rps то он не удерживает.

И тут вступает в силу фильтр Блума!

```
./wrk -c 1 -d 20 -t 1 -L -R 20000 -s get-script.lua http://localhost:8080

Running 20s test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 25.067ms, rate sampling interval: 257ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   684.14us  384.73us   4.10ms   61.93%
    Req/Sec    20.04k    53.91    20.16k    63.16%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%  677.00us
 75.000%    0.99ms
 90.000%    1.17ms
 99.000%    1.63ms
 99.900%    2.25ms
 99.990%    3.83ms
 99.999%    4.10ms
100.000%    4.11ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.018     0.000000            1         1.00
       0.174     0.100000        20017         1.11
       0.301     0.200000        40020         1.25
       0.427     0.300000        60094         1.43
       0.552     0.400000        80041         1.67
       0.677     0.500000       100047         2.00
       0.739     0.550000       110075         2.22
       0.802     0.600000       120003         2.50
       0.865     0.650000       130015         2.86
       0.927     0.700000       139984         3.33
       0.988     0.750000       150061         4.00
       1.019     0.775000       155082         4.44
       1.049     0.800000       160051         5.00
       1.079     0.825000       165021         5.71
       1.108     0.850000       169965         6.67
       1.137     0.875000       174930         8.00
       1.152     0.887500       177582         8.89
       1.165     0.900000       179934        10.00
       1.182     0.912500       182543        11.43
       1.200     0.925000       185059        13.33
       1.218     0.937500       187440        16.00
       1.228     0.943750       188729        17.78
       1.238     0.950000       189931        20.00
       1.250     0.956250       191184        22.86
       1.268     0.962500       192452        26.67
       1.293     0.968750       193675        32.00
       1.309     0.971875       194306        35.56
       1.330     0.975000       194932        40.00
       1.361     0.978125       195544        45.71
       1.410     0.981250       196172        53.33
       1.479     0.984375       196794        64.00
       1.520     0.985938       197108        71.11
       1.558     0.987500       197426        80.00
       1.600     0.989062       197736        91.43
       1.642     0.990625       198050       106.67
       1.679     0.992188       198362       128.00
       1.701     0.992969       198514       142.22
       1.725     0.993750       198673       160.00
       1.750     0.994531       198830       182.86
       1.791     0.995313       198984       213.33
       1.843     0.996094       199137       256.00
       1.872     0.996484       199216       284.44
       1.905     0.996875       199293       320.00
       1.936     0.997266       199371       365.71
       1.993     0.997656       199451       426.67
       2.057     0.998047       199528       512.00
       2.099     0.998242       199569       568.89
       2.147     0.998437       199605       640.00
       2.171     0.998633       199644       731.43
       2.207     0.998828       199683       853.33
       2.257     0.999023       199724      1024.00
       2.303     0.999121       199743      1137.78
       2.339     0.999219       199761      1280.00
       2.491     0.999316       199781      1462.86
       2.743     0.999414       199800      1706.67
       2.919     0.999512       199820      2048.00
       3.079     0.999561       199830      2275.56
       3.217     0.999609       199839      2560.00
       3.383     0.999658       199849      2925.71
       3.513     0.999707       199859      3413.33
       3.657     0.999756       199870      4096.00
       3.691     0.999780       199874      4551.11
       3.705     0.999805       199878      5120.00
       3.759     0.999829       199883      5851.43
       3.783     0.999854       199888      6826.67
       3.807     0.999878       199893      8192.00
       3.827     0.999890       199896      9102.22
       3.833     0.999902       199898     10240.00
       3.853     0.999915       199900     11702.86
       3.873     0.999927       199903     13653.33
       3.913     0.999939       199905     16384.00
       3.955     0.999945       199907     18204.44
       3.979     0.999951       199909     20480.00
       3.979     0.999957       199909     23405.71
       4.001     0.999963       199910     27306.67
       4.023     0.999969       199911     32768.00
       4.045     0.999973       199912     36408.89
       4.057     0.999976       199913     40960.00
       4.057     0.999979       199913     46811.43
       4.073     0.999982       199914     54613.33
       4.073     0.999985       199914     65536.00
       4.099     0.999986       199916     72817.78
       4.099     0.999988       199916     81920.00
       4.099     0.999989       199916     93622.86
       4.099     0.999991       199916    109226.67
       4.099     0.999992       199916    131072.00
       4.099     0.999993       199916    145635.56
       4.099     0.999994       199916    163840.00
       4.099     0.999995       199916    187245.71
       4.107     0.999995       199917    218453.33
       4.107     1.000000       199917          inf
#[Mean    =        0.684, StdDeviation   =        0.385]
#[Max     =        4.104, Total count    =       199917]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  399988 requests in 20.00s, 26.21MB read
Requests/sec:  19999.69
Transfer/sec:      1.31MB
```

Выше видим детализированный вывод wrk2. Все прекрасно. Получается, при большом количестве SSTable без фильтра Блума никак. Он позволяет пропустить таблицы, в которых гарантированно нет ключа. Фильтр вероятностный, реализован в рамках бонусной фичи курса NoSQL. А выше мы видели, как 20k без фильтра сервис просто падал.

Фильтр вероятностный, не вдаваясь в детали, у него есть настраиваемый параметр false-positive probability. В тестах выше он составлял 0.03. То есть только в 3% запросов мы могли сделать лишний бинарный поиск. Получили хорошую оптимизацию (но не бесплатно, конечно же).

Попробуем изменить fpp фильтра на более высокий, чтобы наглядно убедиться как он работает. Зададим fpp = 0.5. Таким образом, в 50% случаев мы сделаем лишний бинарный поиск.

```
./wrk -c 1 -d 20 -t 1 -L -R 20000 -s get-script.lua http://localhost:8080

Running 20s test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 2298.445ms, rate sampling interval: 6889ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     5.61s     1.05s    7.45s    56.39%
    Req/Sec    12.89k     0.00    12.89k     0.00%
----------------------------------------------------------
  250860 requests in 20.00s, 16.40MB read
Requests/sec:  12543.02
Transfer/sec:    839.76KB
```

Видим, что сервис снова упал. Максимальный latency составил 7.45s с фильтром Блума fpp = 0.5. Без фильтра блума он составил примерно в два раза больше - 12.71s. Нетрудно сделать вывод, что фильтр работает. Снижая количество лишних бинарных поисков по таблицам, мы снижаем max latency.

Осталось определить точку разладки с фильтром Блума. Вернем fpp к значению 0.03. Пусть 30k rps:

```
./wrk -c 1 -d 20 -t 1 -L -R 30000 -s get-script.lua http://localhost:8080 

Running 20s test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 944.388ms, rate sampling interval: 2680ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.91s   273.73ms   2.37s    57.52%
    Req/Sec    27.15k     7.54    27.16k    33.33%
----------------------------------------------------------
  529025 requests in 20.00s, 34.71MB read
Requests/sec:  26451.53
Transfer/sec:      1.74MB
```

Сервер захлебнулся. Пусть 25k rps:

```
./wrk -c 1 -d 20 -t 1 -L -R 25000 -s get-script.lua http://localhost:8080 

Running 20s test @ http://localhost:8080
  1 threads and 1 connections
  Thread calibration: mean lat.: 78.520ms, rate sampling interval: 555ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   724.05us  418.22us   4.20ms   68.23%
    Req/Sec    25.02k    31.21    25.09k    72.22%
----------------------------------------------------------
  499981 requests in 20.00s, 32.79MB read
Requests/sec:  24999.19
Transfer/sec:      1.64MB
```

Сервис стабильно работает.

Итак, фильтр Блума с false-positive-probability, равным 0.03, позволил повысить rps с ±7500 до ±25000 для базы с 100 SSTable-ов.

## Профилирование с помощью async-profiler и анализ

# GET

Рассмотрим геты.

# 1) ALLOC

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage1/bf-get-20krps-alloc.png)
<img width="1710" alt="bf-get-20krps-alloc" src="https://github.com/axothy/lsm-dht/assets/61747868/b7ed5b23-fbd5-4677-a954-3740f24e19fa">


Около 80% аллокаций приходится на метод getFromDisk(), а он содержит в себе метод вычисления хеш-функции Murmur3 Hash для ключей. Это используется в фильтре Блума, чтобы определить, лежит ли в данном SSTable такой ключ или нет, для этого вычисляется хеш.

# 2) CPU

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage1/bf-get-20krps-cpu.png)
<img width="1710" alt="bf-get-20krps-cpu" src="https://github.com/axothy/lsm-dht/assets/61747868/84c20a7b-64ae-4018-947a-90edd8293e0a">

Из этого результата профилирования видно что много ресурсов cpu тратится на работу селекторов. На уровне базы данных - все тот же метод getFromDisk() является труднозатратным.

# UPSERT

Рассмотрим вставку-обновление.

# 1) ALLOC

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage1/upsert-30krps-alloc.png)
<img width="1708" alt="upsert-30krps-alloc" src="https://github.com/axothy/lsm-dht/assets/61747868/a7af7577-7644-4967-8730-7613ef41cbf2">

Видно что много аллокаций приходится на SSTablesStorage.write - это как раз сброс данных с memtable на диск. При этом большинство аллокаций возникают в SSTablesStorage.getPaths(). Поскольку этот метод нужен был только для подсчета количества таблиц на диске, метод можно убрать и использовать просто переменную int sstablesCount, которая будет инкрементироваться c каждым flush и становиться равной единице после завершения compaction.

# 2) CPU

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/upsert-30krps-cpu.png)
<img width="1705" alt="upsert-30krps-cpu" src="https://github.com/axothy/lsm-dht/assets/61747868/04d05a99-8d29-4dbc-b103-1fcf9f19e719">

Видно, что затраты процессора вызывает flush (метод SSTablesStorage.write). Метод SSTablesStorage.getPaths() был убран в целях оптимизации.



















## Этап 2. Асинхронный сервер (soft deadline 2024-02-29 18:29:59 MSK, hard deadline 2024-03-06 23:59:59 MSK)

Вынесите **обработку** запросов в отдельный `ExecutorService` с ограниченной очередью, чтобы разгрузить `SelectorThread`ы HTTP сервера. Подумайте о параметрах `ExecutorService` (тип и размер очереди, количество потоков, обработка переполнений очереди и ошибок при обработке запросов) -- результаты всех экспериментов опишите в отчёте. Проанализируйте, стало ли лучше, чем раньше?

Проведите нагрузочное тестирование с помощью [wrk2](https://github.com/giltene/wrk2) с **большим количеством соединений** (не меньше 64) `PUT` и `GET` запросами.

Отпрофилируйте приложение (CPU, alloc и lock) под `PUT` и `GET` нагрузкой с помощью [async-profiler](https://github.com/Artyomcool/async-profiler).

### Report
Когда всё будет готово, присылайте pull request с изменениями, результатами нагрузочного тестирования и профилирования, а также **анализом результатов по сравнению с предыдущей** (синхронной) версией. На всех этапах **оценивается и код, и анализ (отчёт)** -- без анализа полученных результатов работа оценивается минимальным количеством баллов. Не забывайте **отвечать на комментарии в PR** (в том числе автоматизированные) и **исправлять замечания**!

# Отчёт по "Этап 2. Асинхронный сервер"

### Количество потоков в пуле

Для начала выберем количество потоков в пуле воркеров и подходящий размер очереди - эти параметры будем подбирать путем нагрузочного тестирования GET-запросами на поиск случайных ключей в LSM-DAO.

Если сравнивать ArrayBlockingQueue и LinkedBlockingQueue, то просматривая исходный код LinkedBlockingQueue можно сказать, что он использует 2 отдельных лока: для put/offer и для take. ArrayBlockingQueue использует один лок для вставки и удаления и просто не допускает возможности двойного лока. Но в LinkedBlockingQueue связанный список, соответственно операции с этой очередью, скорее всего, будут медленнее чем с ArrayBlockingQueue. Таким образом, динамическое создание узлов при каждой вставке в LinkedBlockingQueue подталкивает меня к использованию ArrayBlockingQueue.

Количество рабочей очереди и размер пула потоков должны соответствовать друг другу. Можно, например, подобрать наиболее оптимальное их отношение.

Подберем сначала размер пула. Можно просто протестировать под нагрузкой с помощью wrk2 - регулировать размер пула, выполняя запросы, понаблюдать за задействованностью процессоров. В общем, размер пула потоков - оценочная величина, и примерное значение может быть подобрано путем профилирования.

Есть одна формула:

N_потоков = N_cpu * U_cpu * (1 + W/C),

где N_cpu - Runtime.getRuntime().availableProcessors(), U_cpu - целевая задействованность процессоров (0 <= U_cpu <= 1), W/C - отношение времени ожидания ко времени вычисления. Чем больше W/C - тем дольше потоки будут ждать чтения sstable.

W в нашем случае - это обращение к диску для чтения файла SSTable. Тут уже зависит от того, есть ли фильтр Блума или нет, ожидание чтения может быть долгим. В моей реализации dao имеется фильтр Блума, поэтому чтение с диска максимально быстрое в связи с сокращением операций бинарного поиска. Можно взять примерно равным 10ms. Это все измеряется.

Будем регулировать как угодно, лишь бы оптимальное значение подобрать, в моем случае для целевой задействованности процессоров на 80% оценка размер пула будет:

N_потоков = 8 * 0.8 * (1 + 10/5) = 19,2 то есть 20 потоков. Начнем именно с этого значения.

База данных предварительно заполнена случайными значениями, количество SSTable в LSM равняется 100. Возьмем нагрузку 100k rps в 4 потока и 64 соединения в wrk2 get-запросами со случайными ключами. Составим таблицу с максимальным latency, средним latency, 90-ым и 99-ым перцентилями.

| POOL SIZE | MAX LATENCY, ms | AVG LATENCY, ms | 90p LATENCY, ms | 99p LATENCY, ms |
|-----------|-----------------|-----------------|-----------------|-----------------|
| 8         | 44.29 | 1.71             | 3.38 | 7.46           |
| 10        | 39.81 | 1.65             | 3.23 | 7.85           |
| 12        | 26.91 | 1.76             | 3.63 | 8.96           |
| 14        | 38.08 | 1.83             | 3.79 | 9.78           |
| 16        | 32.40 | 1.75             | 3.56 | 9.42           |
| 18        | 22.22 | 1.90             | 4.12 | 10.32          |
| 20        | 31.84 | 2.11      <br/>       | 4.66 | 13.50          |
| 22        | 28.98 | 1.80             | 3.67 | 9.72           |
| 24        | 34.94 | 1.81             | 3.69 | 9.51           |
| 32        | 122.62 | 2.95             | 6.02 | 31.97         |
| 64        | 1560 | 20.06             | 8.03 | 755.71         |

Построим график зависимости latency от pool size. В

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/latency(threads).png)
<img width="1585" alt="latency(threads)" src="https://github.com/axothy/lsm-dht/assets/61747868/b8f353ee-0202-4c88-907f-a826fa2c111b">

Тот же график, только с исключением значения 64 по оси x для более детальной визуализации.

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/latency(threads)2.png)
<img width="1588" alt="latency(threads)2" src="https://github.com/axothy/lsm-dht/assets/61747868/b0b6ab2c-5901-4a02-be3a-d0f2ff8cc6f8">

Отсюда вывод, что значение количества потоков, рассчитаное нами по приближенной формуле дает хороший результ. После нагрузочного тестирования было определено оценочное значения для размера пула потоков и оно равняется 20. Исключим из графика кривую MAX LATENCY, чтобы более подробно рассмотреть кривые перцентилей.

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/latency(threads)3.png)
<img width="1577" alt="latency(threads)3" src="https://github.com/axothy/lsm-dht/assets/61747868/b4a5e358-a842-4002-9e98-2e198aa7bae8">

Теперь кривые перцентилей видно лучше.


### Размер очереди пула потоков

Имеет смысл сделать аналогичные замеры и для размера очереди. Предыдущее исследование проводилось для очереди размером 256. Зафиксируем пул из 20 потоков и будем менять размер очереди. Снова проведем нагрузочное тестирование

| QUEUE CAPACITY | MAX LATENCY, ms | AVG LATENCY, ms | 90p LATENCY, ms | 99p LATENCY, ms |
|----------------|-----------------|-----------------|-----------------|-----------------|
| 512            | 3740            | 787.75          | 2090            | 3650            |
| 256            | 27.97           | 1.96            | 4.33            | 11.33           |
| 128            | 75.26           | 1.90            | 3.47            | 13.86           |
| 64             | 21.58           | 1.89            | 3.63            | 8.96            |
| 32             | 8500            | 3050            | 5780            | 7820            |

Из результатов тестирования видно, что слишком маленькая очередь (32) и слишком большая (512) приводят к тому, что сервер захлебывается.

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/latency(queue size).png)
<img width="1586" alt="latency(queue size)" src="https://github.com/axothy/lsm-dht/assets/61747868/7439082d-93d8-4453-ba63-d4b5cbcb70aa">

Тот же график, только в логарифмическом масштабе

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/latency(queue size)2.png)
<img width="1582" alt="latency(queue size)2" src="https://github.com/axothy/lsm-dht/assets/61747868/5571b06d-01d0-49a3-9702-68f96e0eb2bb">

Таким образом, приемлемой будет для нас значит очередь размера 128-256. Ниже вывод wrk2 при нагрузке в 100k rps в 64 соединения и 4 треда на сервер с пулом потоков 20 и очередью 128:

```

./wrk -c 64 -d 20 -t 4 -L -R 100000 -s get-script.lua http://localhost:8080
Running 20s test @ http://localhost:8080
4 threads and 64 connections
Thread calibration: mean lat.: 423.851ms, rate sampling interval: 1651ms
Thread calibration: mean lat.: 515.075ms, rate sampling interval: 1926ms
Thread calibration: mean lat.: 450.551ms, rate sampling interval: 1750ms
Thread calibration: mean lat.: 391.568ms, rate sampling interval: 1662ms
Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency     1.64ms    1.59ms  45.60ms   89.97%
Req/Sec    25.01k    26.85    25.06k    57.14%
Latency Distribution (HdrHistogram - Recorded Latency)
50.000%    1.20ms
75.000%    1.98ms
90.000%    3.23ms
99.000%    7.67ms
99.900%   13.94ms
99.990%   36.64ms
99.999%   44.80ms
100.000%   45.63ms

Detailed Percentile spectrum:
Value   Percentile   TotalCount 1/(1-Percentile)

       0.027     0.000000            2         1.00
       0.458     0.100000        99435         1.11
       0.644     0.200000       198508         1.25
       0.822     0.300000       297629         1.43
       1.003     0.400000       396975         1.67
       1.200     0.500000       496041         2.00
       1.310     0.550000       545932         2.22
       1.434     0.600000       595412         2.50
       1.582     0.650000       644987         2.86
       1.760     0.700000       694430         3.33
       1.977     0.750000       744165         4.00
       2.103     0.775000       768801         4.44
       2.247     0.800000       793754         5.00
       2.415     0.825000       818622         5.71
       2.621     0.850000       843194         6.67
       2.885     0.875000       868062         8.00
       3.045     0.887500       880448         8.89
       3.235     0.900000       892850        10.00
       3.455     0.912500       905179        11.43
       3.723     0.925000       917597        13.33
       4.049     0.937500       930024        16.00
       4.243     0.943750       936171        17.78
       4.459     0.950000       942418        20.00
       4.715     0.956250       948618        22.86
       5.007     0.962500       954789        26.67
       5.363     0.968750       961008        32.00
       5.567     0.971875       964084        35.56
       5.799     0.975000       967197        40.00
       6.055     0.978125       970266        45.71
       6.363     0.981250       973379        53.33
       6.735     0.984375       976485        64.00
       6.947     0.985938       978016        71.11
       7.203     0.987500       979578        80.00
       7.483     0.989062       981124        91.43
       7.815     0.990625       982679       106.67
       8.223     0.992188       984215       128.00
       8.455     0.992969       984994       142.22
       8.711     0.993750       985780       160.00
       8.999     0.994531       986553       182.86
       9.327     0.995313       987314       213.33
       9.743     0.996094       988096       256.00
       9.967     0.996484       988481       284.44
      10.255     0.996875       988864       320.00
      10.591     0.997266       989252       365.71
      11.023     0.997656       989641       426.67
      11.655     0.998047       990028       512.00
      12.031     0.998242       990225       568.89
      12.407     0.998437       990413       640.00
      12.895     0.998633       990611       731.43
      13.479     0.998828       990803       853.33
      13.991     0.999023       990997      1024.00
      14.271     0.999121       991093      1137.78
      14.687     0.999219       991188      1280.00
      15.351     0.999316       991285      1462.86
      16.263     0.999414       991381      1706.67
      17.295     0.999512       991479      2048.00
      18.207     0.999561       991528      2275.56
      19.407     0.999609       991576      2560.00
      21.919     0.999658       991623      2925.71
      24.831     0.999707       991672      3413.33
      27.919     0.999756       991720      4096.00
      29.503     0.999780       991745      4551.11
      31.039     0.999805       991769      5120.00
      32.591     0.999829       991793      5851.43
      34.047     0.999854       991818      6826.67
      35.519     0.999878       991841      8192.00
      36.319     0.999890       991855      9102.22
      36.799     0.999902       991866     10240.00
      37.439     0.999915       991878     11702.86
      38.079     0.999927       991890     13653.33
      38.815     0.999939       991902     16384.00
      39.359     0.999945       991908     18204.44
      39.807     0.999951       991914     20480.00
      40.479     0.999957       991920     23405.71
      41.311     0.999963       991926     27306.67
      41.983     0.999969       991932     32768.00
      42.335     0.999973       991935     36408.89
      42.495     0.999976       991939     40960.00
      42.751     0.999979       991941     46811.43
      43.359     0.999982       991944     54613.33
      43.871     0.999985       991947     65536.00
      44.287     0.999986       991949     72817.78
      44.511     0.999988       991950     81920.00
      44.799     0.999989       991952     93622.86
      44.959     0.999991       991953    109226.67
      44.991     0.999992       991955    131072.00
      45.023     0.999993       991956    145635.56
      45.023     0.999994       991956    163840.00
      45.151     0.999995       991957    187245.71
      45.279     0.999995       991958    218453.33
      45.439     0.999996       991960    262144.00
      45.439     0.999997       991960    291271.11
      45.439     0.999997       991960    327680.00
      45.439     0.999997       991960    374491.43
      45.439     0.999998       991960    436906.67
      45.471     0.999998       991961    524288.00
      45.471     0.999998       991961    582542.22
      45.471     0.999998       991961    655360.00
      45.471     0.999999       991961    748982.86
      45.471     0.999999       991961    873813.33
      45.631     0.999999       991962   1048576.00
      45.631     1.000000       991962          inf
#[Mean    =        1.637, StdDeviation   =        1.591]
#[Max     =       45.600, Total count    =       991962]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
1996022 requests in 20.00s, 130.92MB read
Requests/sec:  99803.54
Transfer/sec:      6.55MB

```

Сервер выдержал нагрузку PUT-запросами 100k rps. Дальнейшем тестированием была определена точка разладки (130k rps), а в целом сервер выдерживает и 120k rps. Если сравнивать с синхронной реализацией, то пул воркеров помог значительно увеличить предельный rps (порог на флаш использовался тот же - 4 MB). Насчет GET-запросов - ситуация аналогичная, асинхронный сервер дает гораздо больший rps.

### Сравнение синхронной и асинхронной реализации сервера


| | SYNCHRONOUS |  ASYNCHRONOUS    | 
|--|----------------|-----------------|
| PUT MAX RPS            | 40k            | 130k |
| GET MAX RPS            | 30k            |  100k |


### Профилирование

#### PUT, cpu

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/profile-put-cpu.png)
<img width="1705" alt="profile-put-cpu" src="https://github.com/axothy/lsm-dht/assets/61747868/8718e253-5b2a-4038-9597-2c2ea3f65268">

Если смотреть на профиль CPU, то видно что ресурсы процессора тратятся на работу ThreadPoolExecutor. Если смотреть дальше - то видно, что ресурсы процессора тратятся на ArrayBlockingQueue take() (~15%) для взятия задачи. Но это мало по сравнению с тем, какую пропускную способность дает ThreadPoolExecutor. Все точно также на профиле виден затрат ресурсов процессора на флаш, как и в прошлом этапе.

#### PUT, alloc

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/profile-put-alloc.png)
<img width="1709" alt="profile-put-alloc" src="https://github.com/axothy/lsm-dht/assets/61747868/1f3b0e39-cdd5-447b-845c-7ca18ec3f5c7">

Сравнивая профиль с прошлым этапом, здесь мало что изменилось. Также на этом этапе были пофикшены аллокации в одном месте, где использовались стримы во время флаша для подсчета количества имеющихся sstable на диске. Теперь стримов нет и аллокаций соответственно тоже.

#### PUT, lock

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/profile-put-lock.png)
<img width="1708" alt="profile-put-lock" src="https://github.com/axothy/lsm-dht/assets/61747868/2a34b548-2d2a-4bb7-af2c-2854edfa32bd">

Видно, что локи возникают в ArrayBlockingQueue (а также есть еще ReentrantReadWriteLock при upsert в dao - он был и в синхронной реализации).
ThreadPoolExecutor - 83.19% локов, SelectorThread - 16.81% локов.


#### GET, cpu

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/profile-get-cpu.png)
<img width="1710" alt="profile-get-cpu" src="https://github.com/axothy/lsm-dht/assets/61747868/926fa2a8-e14e-4674-b96f-b3cde9880ced">

Снова видна работа ThreadPoolExecutor появившегося в асинхронной реализации.

#### GET, alloc

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/profile-get-alloc.png)
<img width="1710" alt="profile-get-alloc" src="https://github.com/axothy/lsm-dht/assets/61747868/62175af7-19e1-4a83-a820-902b5cd319ec">

Визуально - на профилей отличий нет, если сравнивать с синхронной реализацией.

#### GET, lock

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage2/profile-get-lock.png)
<img width="1710" alt="profile-get-lock" src="https://github.com/axothy/lsm-dht/assets/61747868/1ded39b6-019f-4b9e-ab75-600da575ed07">

Здесь локов у ThreadPoolExecutor и SelectorThread 55 на 45.







## Этап 3. Шардирование (soft deadline 2024-03-14 18:29:59 MSK, hard deadline 2024-03-20 23:59:59 MSK)

Реализуем горизонтальное масштабирование через поддержку **кластерных конфигураций**, состоящих из нескольких узлов, взаимодействующих друг с другом через реализованный HTTP API.
Для этого в `ServiceConfig` передаётся статическая "топология", представленная в виде множества координат **всех** узлов кластера в формате `http://<host>:<port>`.

Кластер распределяет ключи между узлами **детерминированным образом**.
В кластере хранится **только одна** копия данных.
Нода, получившая запрос, **проксирует** его на узел, отвечающий за обслуживание соответствующего ключа.
Таким образом, общая ёмкость кластера равна суммарной ёмкости входящих в него узлов.

Реализуйте один из алгоритмов распределения данных между узлами, например, [consistent hashing](https://en.wikipedia.org/wiki/Consistent_hashing), [rendezvous hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing) или что-то другое по согласованию с преподавателем.

### Report
Когда всё будет готово, присылайте pull request с изменениями, результатами нагрузочного тестирования и профилирования, а также **анализом результатов по сравнению с предыдущей** (нераспределённой) версией.
На всех этапах **оценивается и код, и анализ (отчёт)** -- без анализа полученных результатов работа оценивается минимальным количеством баллов.
Не забывайте **отвечать на комментарии в PR** (в том числе автоматизированные) и **исправлять замечания**!
С учётом шардирования набор тестов расширяется, поэтому не забывайте **подмёрдживать upstream**.


# Отчёт по "Этап 3. Шардирование"

### Хеш-функция: насколько данные распределяются равномерно?

В качестве хеш-функции для распределения по нодам была выбрана Murmur3, реализация взята из Cassandra и немного переделана. Обеспечивает более-менее равномерное распределение.

Протестируем равномерность распределения. При помощи wrk2 нагрузим нашу базу PUT-запросами. Будем использовать 10 нод, чтобы лучше увидеть как данные распределяются между нодами. После нагрузки вычислим размер занимаемых SSTable каждой ноды. Ключи, генерируемые Lua скриптом, случайные.

```
du -sh *

80M	tmp.8080
80M	tmp.8081
80M	tmp.8082
79M	tmp.8083
79M	tmp.8084
80M	tmp.8085
79M	tmp.8086
79M	tmp.8087
79M	tmp.8088
81M	tmp.8089
```

Вывод: исходя из размера файлов на диске, распределение достаточно равномерно.


### Latency: что с ним стало?

В дальнейших исследованиях будем использовать кластер из 3-ех нод. Порог на flush тот же самый - 4 МБ. На прошлом этапе база без шардирования давала следующие показатели (точки разладки):



| | SYNCHRONOUS |  ASYNCHRONOUS    | 
|--|----------------|-----------------|
| PUT MAX RPS            | 40k            | 130k |
| GET MAX RPS            | 30k            |  100k |



В случае с добавлением шардирования мы должны ожидать снижение RPS который может выдерживать сервер, связанное с необходимостью сервера проксировать запросы другим нодам по HTTP. Кроме того, если в кластере будет большое количество нод, то вероятность попасть сразу в нужную ноду снижается. (1/N вероятность попасть сразу в нужную ноду в предположении что ключи там распределены равномерно).

Много общаться с другими нодами мы не хотим поэтому пусть их число будет 3.

Детательным регулированием нагрузки с шагом в 500 rps была определена точка разладки для PUT-запросов - где-то в окрестности значения 18 500 rps:

```
./wrk -c 64 -d 20 -t 4 -L -R 18500 -s upsert-script.lua http://localhost:8080


Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency    10.31ms   45.81ms 712.70ms   95.95%
Req/Sec     4.85k    13.38     4.87k    50.00%

Latency Distribution (HdrHistogram - Recorded Latency)
90.000%    3.09ms
99.000%  266.24ms
99.900%  509.18ms
99.990%  672.77ms
99.999%  709.63ms
100.000%  713.22ms
```

Точно таким же образом была определена точка разладки и для GET-запросов: где-то в окрестности 17 000 rps:

```
./wrk -c 64 -d 20 -t 4 -L -R 17000 -s get-script.lua http://localhost:8080


Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency     2.10ms    3.68ms  42.85ms   94.45%
Req/Sec     4.25k     0.00     4.25k
     0.00%
Latency Distribution (HdrHistogram - Recorded Latency)
90.000%    2.52ms
99.000%   22.16ms
99.900%   34.62ms
99.990%   39.30ms
99.999%   41.02ms
100.000%   42.88ms
```

*Замечание*. Для GET-запросов база данных наполнялась почти таким же образом, как и в прошлых этапах:

1) Генерировался случайный и уникальный набор ключей и значений (k,v).
2) Фиксировалось заранее количество SSTable в каждой ноде.
3) База наполняется этими (k,v) с учетом хеширования, при этом происходят флаши в каждой ноде

Таким образом, после инициализации мы имеем заполненную базу данных с тремя нодами, а количество таблиц на диске у каждой ноды примерно равно заданному (в моем случае я выбрал по 100 таблиц на ноду). База наполняется программно. Далее ее можно тестировать GET-запросами. Такое наполнение базы является "честным".

Обработка GET-запросов опять же не сильно отстает от PUT-запросов, поскольку в мои SSTable встроен фильтр Блума. Итого:

| | SYNCHRONOUS |  ASYNCHRONOUS    | ASYNCHRONOUS + SHARDING |
|--|----------------|-----------------|-------------------------|
| PUT MAX RPS            | 40k            | 130k | 18 500                  |
| GET MAX RPS            | 30k            |  100k | 17 000               |   


### Оптимизации

Попробуем применить некоторые оптимизации.
Рассмотрим профили под нагрузкой. Посмотрим что можно соптимизировать а в конце проведем итоговый анализ.

Профиль приложения под PUT-нагрузкой, CPU:

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-test-put-cpu-v1.png)
<img width="1694" alt="profile-test-put-cpu-v1" src="https://github.com/axothy/lsm-dht/assets/61747868/0eb0a385-0e91-4e3d-87f3-8285b6630ac7">

Профиль приложения под PUT-нагрузкой, alloc:

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-test-put-alloc-v1.png)
<img width="1707" alt="profile-test-put-alloc-v1" src="https://github.com/axothy/lsm-dht/assets/61747868/292b2ffb-b75f-4362-8844-082ef7c3470a">

Из профилей выше видно, что в методе `selectPartition()` есть аллокации (7.35%), где в методе `fromString()` происходит 3.33% из них. Это метод, который из строки получает MemorySegment а далее хеширует сегмент:

```
    private static MemorySegment fromString(String data) {
        return MemorySegment.ofArray(data.getBytes(StandardCharsets.UTF_8));
    }
```

Избавимся от него, таким образом будем хешировать сразу строку вместо MemorySegment. Отпрофилируем заново в аналогичных условиях:

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-test-put-alloc-v2.png)
<img width="1703" alt="profile-test-put-alloc-v2" src="https://github.com/axothy/lsm-dht/assets/61747868/c4da8b13-956f-418c-a953-17c48f967d7e">

Таким образом, после оптимизации я сократил аллокации возникающие в алгоритме хеширования с 7.35% до 3.78%.

Еще насчет оптимизаций - можно было бы не считать хеш два раза, а например сразу класть в ноду entry когда она прилетает от мастер ноды.


### Анализ профилирования


#### PUT, cpu

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-put-cpu.png)
<img width="1709" alt="profile-put-cpu" src="https://github.com/axothy/lsm-dht/assets/61747868/28d7a936-a580-4906-93f5-c5f42e88275f">

Сравнивая данный профиль с нераспределенной версией, к тред пулу и селектор тредам добавляется `HttpClientImpl$SelectorManager`, достающий из очереди задачи клиента. Также добавляются `InternalWriteSubscriber` и `SequentialScheduler.SchedulableTask` из пакета `jdk.http`.

#### PUT, alloc

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-put-alloc.png)
<img width="1710" alt="profile-put-alloc" src="https://github.com/axothy/lsm-dht/assets/61747868/013ed968-dffb-4c2e-9261-c2a597d2f1f7">

Сравнивая данный профиль с нераспределенной версией, очевидно появились аллокации в селектор треде связанные с проксированием запроса другой ноде.

#### PUT, lock

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-put-lock.png)
<img width="1710" alt="profile-put-lock" src="https://github.com/axothy/lsm-dht/assets/61747868/8b20502f-ce98-446f-9b02-b20a056d4614">

Сравнивая данный профиль с нераспределенной версией, где в SelectorThread было 16.81% локов, сейчас их там всего 1.94%. Но добавилось 14.79% локов в `HttpClientImpl$SelectorManager`.

#### GET, cpu

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-get-cpu.png)
<img width="1710" alt="profile-get-cpu" src="https://github.com/axothy/lsm-dht/assets/61747868/281eff83-56ce-4d13-bc6a-9dae3ecdeeeb">

Сравнивая данный профиль с нераспределенной версией, как и в PUT-запросал появился `HttpClientImpl$SelectorManager`.

#### GET, alloc

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-get-alloc.png)
<img width="1710" alt="profile-get-alloc" src="https://github.com/axothy/lsm-dht/assets/61747868/8edc3690-4532-45f5-b977-5546b3b358c6">



#### GET, lock

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage3/imgs/profile-get-lock.png)
<img width="1710" alt="profile-get-lock" src="https://github.com/axothy/lsm-dht/assets/61747868/472896d2-0344-434c-b829-f1447010e2ec">












## Этап 4. Репликация (soft deadline 2024-03-28 18:29:59 MSK, hard deadline 2024-04-03 23:59:59 MSK)

Реализуем поддержку хранения [нескольких реплик](https://en.wikipedia.org/wiki/Replication_(computing)) данных в кластере для обеспечения отказоустойчивости.

HTTP API расширяется query-параметрами `from` и `ack`, содержащими количество узлов, которые должны подтвердить операцию, чтобы она считалась выполненной успешно.
* `ack` -- сколько ответов нужно получить
* `from` -- от какого количества узлов

Таким образом, теперь узлы должны поддерживать расширенный протокол (совместимый с предыдущей версией):
* HTTP `GET /v0/entity?id=<ID>[&ack=<ACK>&from=<FROM>]` -- получить данные по ключу `<ID>`. Возвращает:
    * `200 OK` и данные, если ответили хотя бы `ack` из `from` реплик
    * `404 Not Found`, если ни одна из `ack` реплик, вернувших ответ, не содержит данные (либо **самая свежая версия** среди `ack` ответов -- это tombstone)
    * `504 Not Enough Replicas`, если не получили `200`/`404` от `ack` реплик из всего множества `from` реплик

* HTTP `PUT /v0/entity?id=<ID>[&ack=<ACK>&from=<FROM>]` -- создать/перезаписать (upsert) данные по ключу `<ID>`. Возвращает:
    * `201 Created`, если хотя бы `ack` из `from` реплик подтвердили операцию
    * `504 Not Enough Replicas`, если не набралось `ack` подтверждений из всего множества `from` реплик

* HTTP `DELETE /v0/entity?id=<ID>[&ack=<ACK>&from=<FROM>]` -- удалить данные по ключу `<ID>`. Возвращает:
    * `202 Accepted`, если хотя бы `ack` из `from` реплик подтвердили операцию
    * `504 Not Enough Replicas`, если не набралось `ack` подтверждений из всего множества `from` реплик

Если параметр `replicas` не указан, то в качестве `ack` используется значение по умолчанию, равное **кворуму** от количества узлов в кластере, а `from` равен общему количеству узлов в кластере, например:
* `1/1` для кластера из одного узла
* `2/2` для кластера из двух узлов
* `2/3` для кластера из трёх узлов
* `3/4` для кластера из четырёх узлов
* `3/5` для кластера из пяти узлов

Выбор узлов-реплик (множества `from`) для каждого `<ID>` является **детерминированным**:
* Множество узлов-реплик для фиксированного ID и меньшего значения `from` является строгим подмножеством для большего значения `from`
* При `PUT` не сохраняется больше копий данных, чем указано в `from` (т.е. не стоит писать лишние копии данных на все реплики)

Фактически, с помощью параметра `replicas` клиент выбирает, сколько копий данных он хочет хранить, а также
уровень консистентности при выполнении последовательности операций для одного ID.

Таким образом, обеспечиваются следующие примеры инвариантов (список не исчерпывающий):
* `GET` с `1/2` всегда вернёт данные, сохранённые с помощью `PUT` с `2/2` (даже при недоступности одной реплики при `GET`)
* `GET` с `2/3` всегда вернёт данные, сохранённые с помощью `PUT` с `2/3` (даже при недоступности одной реплики при `GET`)
* `GET` с `1/2` "увидит" результат `DELETE` с `2/2` (даже при недоступности одной реплики при `GET`)
* `GET` с `2/3` "увидит" результат `DELETE` с `2/3` (даже при недоступности одной реплики при `GET`)
* `GET` с `1/2` может не "увидеть" результат `PUT` с `1/2`
* `GET` с `1/3` может не "увидеть" результат `PUT` с `2/3`
* `GET` с `1/2` может вернуть данные несмотря на предшествующий `DELETE` с `1/2`
* `GET` с `1/3` может вернуть данные несмотря на предшествующий `DELETE` с `2/3`
* `GET` с `ack` равным `quorum(from)` "увидит" результат `PUT`/`DELETE` с `ack` равным `quorum(from)` даже при недоступности **<** `quorum(from)` реплик


# Отчёт по "Этап 4. Репликация"

### Формат SSTable

В случае конфликтов, когда ноды возвращают разные ответы, был использован timestamp, таким образом
реализован last write wins. Формат SSTable был изменен для хранения timestamp.

SStable Header следующего формата:

| 8                     | 8                      | 8               |
|-----------------------|------------------------|-----------------|
| `Bloom filter length` | `Hash functions count` | `entries count` |

SStable Bloom filter следующего формата:

| 8      | 8      | ...       | 8      |
|--------|--------|-----------|--------|
| `hash_0` | `hash_1` | `hash_i...` | `hash_n` |

где `n` - размер фильтра Блума (`Bloom filter length`)

| 8 x entries count    | 8         | key size | 8           | value size | 8         |
|----------------------|-----------|----------|-------------|------------|-----------|
| `key_i offset` | `key size` | `key`      | `value size` | `value`      | `timestamp` |

где `i = 0, 1, ..., entries count`

Если же наша `entry` это `tombstone`, то `entry` записывается в следующем формате:

| 8         | key size | 8   | 8           |
|-----------|----------|-----|-------------|
| `key size` | key      | `-1` | `timestamp` |

где `-1` это tombstone tag. Таким образом в случае могилки, value не сохранится при `flush`, а `-1` подставляется на место value size.

Timestamp всегда фиксируется в мастер ноде и далее в случае proxy запроса передается вместе с proxy запросом другой ноде.


### Latency: что с ним стало?

В дальнейших исследованиях будем использовать кластер из 3-ех нод, `ack/from = 2/3`. Порог на flush тот же самый - `4 МБ`. На прошлом этапе база без репликации давала следующие показатели (точки разладки):

|             | SYNCHRONOUS | ASYNCHRONOUS | ASYNCHRONOUS + SHARDING |
|-------------|-------------|--------------|-------------------------|
| PUT MAX RPS | 40k         | 130k         | 18 500                  |
| GET MAX RPS | 30k         | 100k         | 17 000                  |   


Детательным регулированием нагрузки с шагом в 500 rps была определена точка разладки для PUT-запросов:

```
./wrk -c 64 -d 20 -t 4 -L -R 1200 -s upsert-script.lua http://localhost:8080


  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.66ms    6.57ms  64.35ms   94.33%
    Req/Sec   308.50    129.79   380.00     83.33%
  Latency Distribution (HdrHistogram - Recorded Latency)
 90.000%    5.54ms
 99.000%   38.02ms
 99.900%   60.45ms
 99.990%   64.32ms
 99.999%   64.38ms
100.000%   64.38ms
```

Latency сильно просел, теперь точка разладки для PUT запросов составляет всего 1200 rps.

Точно таким же образом была определена точка разладки и для GET-запросов:

```
./wrk -c 64 -d 20 -t 4 -L -R 400 -s get-script.lua http://localhost:8080


  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     6.43ms   10.05ms  80.77ms   92.93%
    Req/Sec    73.22     57.87   129.00     62.50%
  Latency Distribution (HdrHistogram - Recorded Latency)
 90.000%   11.86ms
 99.000%   57.66ms
 99.900%   78.46ms
 99.990%   80.83ms
 99.999%   80.83ms
100.000%   80.83ms
```

У нас в несколько раз просел rps, таким образом имеем следующее:

|             | SYNCHRONOUS | ASYNCHRONOUS | ASYNCHRONOUS + SHARDING | ASYNCHRONOUS + SHARDING + REPLICATION |
|-------------|-------------|--------------|-------------------------|---------------------------------------|
| PUT MAX RPS | 40k         | 130k         | 18 500                  | 1200                                  |
| GET MAX RPS | 30k         | 100k         | 17 000                  | 400                                   |

Добавление репликации сыграло критическую роль. Если в кластере будет еще больше нод - rps соответственно снизится.
При дефолтных значениях параметров ack/from мы больше общаемся с другими нодами - возрастают затраты времени на
отправку запросов и получение ответов. В случае GET запросов серверу необходимо в `ack` раз больше общаться по HTTP с другими нодами.


### Анализ профилирования


#### PUT, cpu

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage4/imgs/profile-cpu-put.png)
<img width="1710" alt="profile-cpu-put" src="https://github.com/axothy/lsm-dht/assets/61747868/b75ec8d8-5916-4618-a2cd-3095c84803ad">

Сравнивая данный профиль с версией без репликации, особых отличий нет, единственное, на профиле cpu реплицированной
версии видно, что ThreadPoolExecutor выполняет еще и отправку запросов другим нодам, помимо локальной обработки запроса.


#### PUT, alloc

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage4/imgs/profile-alloc-put.png)
<img width="1710" alt="profile-alloc-put" src="https://github.com/axothy/lsm-dht/assets/61747868/36cdc07a-1549-4f6b-a7fe-6dff0f807545">

Сравнивая данный профиль с версией без репликации, особых отличий не наблюдается.
Заметно, что в SelectorThread аллокации увеличились примерно на 2%. Это связано с методом
`parseTimestamp` для того, чтобы из заголовка ответа достать timestamp и далее производить сравнения
ответов разных нод.

#### PUT, lock

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage4/imgs/profile-lock-put.png)
<img width="1709" alt="profile-lock-put" src="https://github.com/axothy/lsm-dht/assets/61747868/ad1dee07-eced-401c-b576-1b8908f258aa">

Видно что в версии с репликацией значительно добавились локи при обработке запроса.
Можно улучшить данное решение, если внутреннее сетевое взаимодействие узлов сделать асинхронным,
таким образом, если потоки не будут блокироваться, данный профиль изменится.

А сейчас, чем больше задач приходит воркерам, тем больше локов.

#### GET, cpu

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage4/imgs/profile-cpu-get.png)
<img width="1710" alt="profile-cpu-get" src="https://github.com/axothy/lsm-dht/assets/61747868/f9e4507e-6a1e-4aa1-a93d-c99663ca298b">

Особых изменений не наблюдается.

#### GET, alloc

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage4/imgs/profile-alloc-get.png)
<img width="1706" alt="profile-alloc-get" src="https://github.com/axothy/lsm-dht/assets/61747868/9a557553-9a0e-4dd5-9462-c91da662c2b2">

Здесь хорошо заметно что увеличились аллокации в пуле воркеров, связано это с общением с другими нодами.
Касательно того что под капотом Dao: во время профилирования база нагружалась
GET-запросами с `ack/from = 2/3`, соответственно здесь немного больше работы по поиску на диске. Но все же эти аллокации не увеличились так, как аллокации во время работы по сети.

#### GET, lock

![](/Users/axothy/IdeaProjects/lsm-dht/src/main/java/ru/vk/itmo/test/dht/wrk_results/stage4/imgs/profile-lock-get.png)
<img width="1710" alt="profile-lock-get" src="https://github.com/axothy/lsm-dht/assets/61747868/79aaf962-dff3-4fa4-a5c0-eed7ef3f0dd7">

Опять же появилось больше локов, связанных с общением нод.


Подытоживая, имеем заметное ухудшение производительности, но теперь мы данные храним сразу на нескольких нодах, получив большую
отказоустойчивость. Можно поменять протокол на более эффективный,
можно переключить сетевое взаимодействие на асинхронное, чтобы избавиться
от блокировки потоков.








## Этап 5. Асинхронное взаимодействие (soft deadline 2024-04-11 18:29:59 MSK, hard deadline 2024-04-17 23:59:59 MSK)

Переключаем внутреннее сетевое взаимодействие узлов на **асинхронный** `java.net.http.HttpClient` (если ещё нет).
**Параллельно** отправляем запросы репликам и собираем **самые быстрые** ответы на `CompletableFuture`.
Потоки **не должны блокироваться** совсем (кроме синхронных методов локального DAO).

Проведите нагрузочное тестирование с помощью [wrk2](https://github.com/giltene/wrk2) в несколько соединений.

Отпрофилируйте приложение (CPU, alloc и **особенно lock**) под нагрузкой и **сравните** latency и результаты профилирования с предыдущей неасинхронной версией.

### Report
Когда всё будет готово, присылайте pull request с изменениями, результатами нагрузочного тестирования и профилирования, а также **анализом результатов по сравнению с предыдущей** (синхронной) версией.
На всех этапах **оценивается и код, и анализ (отчёт)** -- без анализа полученных результатов работа оценивается минимальным количеством баллов.
Не забывайте **отвечать на комментарии в PR** (в том числе автоматизированные) и **исправлять замечания**!


